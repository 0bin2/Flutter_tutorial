{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0bin2/Flutter_tutorial/blob/main/t5_wsc_base_random_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtj3uso-P1fy",
        "outputId": "2b8d72cc-2625-422d-9a31-d3a7511f74d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece==0.1.94 in /usr/local/lib/python3.10/dist-packages (0.1.94)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.65.0)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
            "Requirement already satisfied: accelerate>=0.20.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.21.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.2->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.22.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece==0.1.94\n",
        "!pip install transformers[torch]\n",
        "!pip install datasets\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwS8yE4hd_Vu",
        "outputId": "ae239505-e989-4326-8b13-f0070810ffec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import transformers"
      ],
      "metadata": {
        "id": "16pESC6cXAVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Iwl7HnWs4Jt",
        "outputId": "5ee31b17-f9a1-4013-bfd7-dbefac3f246d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import nltk\n",
        "import evaluate\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "iDh8FRPtdlwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq,T5ForConditionalGeneration, T5Model,T5Tokenizer, pipeline\n",
        "from datasets import load_dataset, load_metric"
      ],
      "metadata": {
        "id": "sGf_MtyQ49aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.t5.modeling_t5 import *\n",
        "class custom_T5(T5ForConditionalGeneration):\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
        "        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        decoder_head_mask: Optional[torch.FloatTensor] = None,\n",
        "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
        "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "        forward_type: Optional[str] = 'gen'\n",
        "    ):\n",
        "        \"\"\"\n",
        "        finetuned model is trained with conditional generation task(forward_type = gen)\n",
        "        policy use last hidden state of lm(forward_type = lm)\n",
        "        \"\"\"\n",
        "        if forward_type == 'gen':\n",
        "            return self.forward_gen(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                decoder_input_ids=decoder_input_ids,\n",
        "                decoder_attention_mask=decoder_attention_mask,\n",
        "                head_mask=head_mask,\n",
        "                decoder_head_mask=decoder_head_mask,\n",
        "                cross_attn_head_mask=cross_attn_head_mask,\n",
        "                encoder_outputs=encoder_outputs,\n",
        "                past_key_values=past_key_values,\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                decoder_inputs_embeds=decoder_inputs_embeds,\n",
        "                labels=labels,\n",
        "                use_cache=use_cache,\n",
        "                output_attentions=output_attentions,\n",
        "                output_hidden_states=output_hidden_states,\n",
        "                return_dict=return_dict,\n",
        "            )\n",
        "        elif forward_type == 'lm':\n",
        "            return self.forward_lm(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                decoder_input_ids=decoder_input_ids,\n",
        "                decoder_attention_mask=decoder_attention_mask,\n",
        "                head_mask=head_mask,\n",
        "                decoder_head_mask=decoder_head_mask,\n",
        "                cross_attn_head_mask=cross_attn_head_mask,\n",
        "                encoder_outputs=encoder_outputs,\n",
        "                past_key_values=past_key_values,\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                decoder_inputs_embeds=decoder_inputs_embeds,\n",
        "                labels=labels,\n",
        "                use_cache=use_cache,\n",
        "                output_attentions=output_attentions,\n",
        "                output_hidden_states=output_hidden_states,\n",
        "                return_dict=return_dict,\n",
        "            )\n",
        "    def forward_gen(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
        "        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        decoder_head_mask: Optional[torch.FloatTensor] = None,\n",
        "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
        "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in `[-100, 0, ...,\n",
        "            config.vocab_size - 1]`. All labels set to `-100` are ignored (masked), the loss is only computed for\n",
        "            labels in `[0, ..., config.vocab_size]`\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        Examples:\n",
        "\n",
        "        ```python\n",
        "        >>> from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
        "\n",
        "        >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "        >>> model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "        >>> # training\n",
        "        >>> input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
        "        >>> labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
        "        >>> outputs = model(input_ids=input_ids, labels=labels)\n",
        "        >>> loss = outputs.loss\n",
        "        >>> logits = outputs.logits\n",
        "\n",
        "        >>> # inference\n",
        "        >>> input_ids = tokenizer(\n",
        "        ...     \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
        "        ... ).input_ids  # Batch size 1\n",
        "        >>> outputs = model.generate(input_ids)\n",
        "        >>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "        >>> # studies have shown that owning a dog is good for you.\n",
        "        ```\"\"\"\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n",
        "        if head_mask is not None and decoder_head_mask is None:\n",
        "            if self.config.num_layers == self.config.num_decoder_layers:\n",
        "                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n",
        "                decoder_head_mask = head_mask\n",
        "\n",
        "        # Encode if needed (training, first prediction pass)\n",
        "        if encoder_outputs is None:\n",
        "            # Convert encoder inputs in embeddings if needed\n",
        "            encoder_outputs = self.encoder(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                head_mask=head_mask,\n",
        "                output_attentions=output_attentions,\n",
        "                output_hidden_states=output_hidden_states,\n",
        "                return_dict=return_dict,\n",
        "            )\n",
        "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
        "            encoder_outputs = BaseModelOutput(\n",
        "                last_hidden_state=encoder_outputs[0],\n",
        "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
        "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
        "            )\n",
        "\n",
        "        hidden_states = encoder_outputs[0]\n",
        "\n",
        "        if self.model_parallel:\n",
        "            torch.cuda.set_device(self.decoder.first_device)\n",
        "\n",
        "        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n",
        "            # get decoder inputs from shifting lm labels to the right\n",
        "            decoder_input_ids = self._shift_right(labels)\n",
        "\n",
        "        # Set device for model parallelism\n",
        "        if self.model_parallel:\n",
        "            torch.cuda.set_device(self.decoder.first_device)\n",
        "            hidden_states = hidden_states.to(self.decoder.first_device)\n",
        "            if decoder_input_ids is not None:\n",
        "                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n",
        "            if attention_mask is not None:\n",
        "                attention_mask = attention_mask.to(self.decoder.first_device)\n",
        "            if decoder_attention_mask is not None:\n",
        "                decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)\n",
        "\n",
        "        # Decode\n",
        "        decoder_outputs = self.decoder(\n",
        "            input_ids=decoder_input_ids,\n",
        "            attention_mask=decoder_attention_mask,\n",
        "            inputs_embeds=decoder_inputs_embeds,\n",
        "            past_key_values=past_key_values,\n",
        "            encoder_hidden_states=hidden_states,\n",
        "            encoder_attention_mask=attention_mask,\n",
        "            head_mask=decoder_head_mask,\n",
        "            cross_attn_head_mask=cross_attn_head_mask,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = decoder_outputs[0]\n",
        "\n",
        "        # Set device for model parallelism\n",
        "        if self.model_parallel:\n",
        "            torch.cuda.set_device(self.encoder.first_device)\n",
        "            self.lm_head = self.lm_head.to(self.encoder.first_device)\n",
        "            sequence_output = sequence_output.to(self.lm_head.weight.device)\n",
        "\n",
        "        if self.config.tie_word_embeddings:\n",
        "            # Rescale output before projecting on vocab\n",
        "            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n",
        "            sequence_output = sequence_output * (self.model_dim**-0.5)\n",
        "\n",
        "        lm_logits = self.lm_head(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
        "            # move labels to correct device to enable PP\n",
        "            labels = labels.to(lm_logits.device)\n",
        "            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n",
        "            # TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return Seq2SeqLMOutput(\n",
        "            loss=loss,\n",
        "            logits=lm_logits,\n",
        "            past_key_values=decoder_outputs.past_key_values,\n",
        "            decoder_hidden_states=sequence_output,\n",
        "            decoder_attentions=decoder_outputs.attentions,\n",
        "            cross_attentions=decoder_outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
        "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
        "            encoder_attentions=encoder_outputs.attentions,\n",
        "        )\n",
        "    def forward_lm(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
        "        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        decoder_head_mask: Optional[torch.FloatTensor] = None,\n",
        "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        decoder_inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n",
        "        r\"\"\"\n",
        "        Returns:\n",
        "\n",
        "        Example:\n",
        "\n",
        "        ```python\n",
        "        >>> from transformers import AutoTokenizer, T5Model\n",
        "\n",
        "        >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "        >>> model = T5Model.from_pretrained(\"t5-small\")\n",
        "\n",
        "        >>> input_ids = tokenizer(\n",
        "        ...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
        "        ... ).input_ids  # Batch size 1\n",
        "        >>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n",
        "\n",
        "        >>> # preprocess: Prepend decoder_input_ids with start token which is pad token for T5Model.\n",
        "        >>> # This is not needed for torch's T5ForConditionalGeneration as it does this internally using labels arg.\n",
        "        >>> decoder_input_ids = model._shift_right(decoder_input_ids)\n",
        "\n",
        "        >>> # forward pass\n",
        "        >>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
        "        >>> last_hidden_states = outputs.last_hidden_state\n",
        "        ```\"\"\"\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n",
        "        if head_mask is not None and decoder_head_mask is None:\n",
        "            if self.config.num_layers == self.config.num_decoder_layers:\n",
        "                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n",
        "                decoder_head_mask = head_mask\n",
        "\n",
        "        # Encode if needed (training, first prediction pass)\n",
        "        if encoder_outputs is None:\n",
        "            encoder_outputs = self.encoder(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                head_mask=head_mask,\n",
        "                output_attentions=output_attentions,\n",
        "                output_hidden_states=output_hidden_states,\n",
        "                return_dict=return_dict,\n",
        "            )\n",
        "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
        "            encoder_outputs = BaseModelOutput(\n",
        "                last_hidden_state=encoder_outputs[0],\n",
        "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
        "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
        "            )\n",
        "\n",
        "        hidden_states = encoder_outputs[0]\n",
        "\n",
        "        # Set device for model parallelism\n",
        "        if self.model_parallel:\n",
        "            torch.cuda.set_device(self.decoder.first_device)\n",
        "            hidden_states = hidden_states.to(self.decoder.first_device)\n",
        "            if decoder_input_ids is not None:\n",
        "                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n",
        "            if attention_mask is not None:\n",
        "                attention_mask = attention_mask.to(self.decoder.first_device)\n",
        "            if decoder_attention_mask is not None:\n",
        "                decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)\n",
        "\n",
        "        # Decode\n",
        "        decoder_outputs = self.decoder(\n",
        "            input_ids=decoder_input_ids,\n",
        "            attention_mask=decoder_attention_mask,\n",
        "            inputs_embeds=decoder_inputs_embeds,\n",
        "            past_key_values=past_key_values,\n",
        "            encoder_hidden_states=hidden_states,\n",
        "            encoder_attention_mask=attention_mask,\n",
        "            head_mask=decoder_head_mask,\n",
        "            cross_attn_head_mask=cross_attn_head_mask,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        if not return_dict:\n",
        "            return decoder_outputs + encoder_outputs\n",
        "\n",
        "        return Seq2SeqModelOutput(\n",
        "            last_hidden_state=decoder_outputs.last_hidden_state,\n",
        "            past_key_values=decoder_outputs.past_key_values,\n",
        "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
        "            decoder_attentions=decoder_outputs.attentions,\n",
        "            cross_attentions=decoder_outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
        "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
        "            encoder_attentions=encoder_outputs.attentions,\n",
        "        )"
      ],
      "metadata": {
        "id": "rSrtXTfqGd20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# rest"
      ],
      "metadata": {
        "id": "oMHetCOh_TUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "XibcmPWbIalV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from numba import cuda"
      ],
      "metadata": {
        "id": "zE0nUn5cJCN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Nq7MEe8Ipcs",
        "outputId": "a537ddf4-2910-473d-9763-fcdfbc544ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jul 15 02:21:10 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    69W / 400W |   8417MiB / 40960MiB |    100%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# device = cuda.get_current_device()\n",
        "# device.reset()"
      ],
      "metadata": {
        "id": "RTez9GYNIqHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.data.data_collator import *\n",
        "@dataclass\n",
        "class FT_DataCollatorForSeq2Seq:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received, as well as the labels.\n",
        "\n",
        "    Args:\n",
        "        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
        "            The tokenizer used for encoding the data.\n",
        "        model ([`PreTrainedModel`]):\n",
        "            The model that is being trained. If set and has the *prepare_decoder_input_ids_from_labels*, use it to\n",
        "            prepare the *decoder_input_ids*\n",
        "\n",
        "            This is useful when using *label_smoothing* to avoid calculating loss twice.\n",
        "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
        "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
        "            among:\n",
        "\n",
        "            - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n",
        "              sequence is provided).\n",
        "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
        "              acceptable input length for the model if that argument is not provided.\n",
        "            - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n",
        "        max_length (`int`, *optional*):\n",
        "            Maximum length of the returned list and optionally padding length (see above).\n",
        "        pad_to_multiple_of (`int`, *optional*):\n",
        "            If set will pad the sequence to a multiple of the provided value.\n",
        "\n",
        "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
        "            7.5 (Volta).\n",
        "        label_pad_token_id (`int`, *optional*, defaults to -100):\n",
        "            The id to use when padding the labels (-100 will be automatically ignored by PyTorch loss functions).\n",
        "        return_tensors (`str`):\n",
        "            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
        "    \"\"\"\n",
        "\n",
        "    tokenizer: PreTrainedTokenizerBase\n",
        "    model: Optional[Any] = None\n",
        "    policy_selection_model: Optional[Any] = None\n",
        "    padding: Union[bool, str, PaddingStrategy] = True\n",
        "    max_length: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    label_pad_token_id: int = -100\n",
        "    return_tensors: str = \"pt\"\n",
        "\n",
        "    def tokenize_batch(self,prompt_batch,org_examples):\n",
        "        # encode a batch of sentences\n",
        "        assert len(prompt_batch) == len(org_examples)\n",
        "        tokenized_batch = []\n",
        "        for idx, prompt in prompt_batch:\n",
        "            tokenized_prompt = self.tokenize(prompt)\n",
        "            tokenized_prompt['labels'] = org_examples[idx]['labels']\n",
        "            tokenized_batch.append(tokenized_prompt)\n",
        "        return tokenized_batch\n",
        "\n",
        "    def __call__(self, features, return_tensors=None):\n",
        "        # features = self.prompt_selection_policy.get_features_batch(features)\n",
        "        # print(features)\n",
        "        if return_tensors is None:\n",
        "            return_tensors = self.return_tensors\n",
        "        labels = [feature[\"labels\"] for feature in features] if \"labels\" in features[0].keys() else None\n",
        "        # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n",
        "        # same length to return tensors.\n",
        "        if labels is not None:\n",
        "            max_label_length = max(len(l) for l in labels)\n",
        "            if self.pad_to_multiple_of is not None:\n",
        "                max_label_length = (\n",
        "                    (max_label_length + self.pad_to_multiple_of - 1)\n",
        "                    // self.pad_to_multiple_of\n",
        "                    * self.pad_to_multiple_of\n",
        "                )\n",
        "\n",
        "            padding_side = self.tokenizer.padding_side\n",
        "            for feature in features:\n",
        "                remainder = [self.label_pad_token_id] * (max_label_length - len(feature[\"labels\"]))\n",
        "                if isinstance(feature[\"labels\"], list):\n",
        "                    feature[\"labels\"] = (\n",
        "                        feature[\"labels\"] + remainder if padding_side == \"right\" else remainder + feature[\"labels\"]\n",
        "                    )\n",
        "                elif padding_side == \"right\":\n",
        "                    feature[\"labels\"] = np.concatenate([feature[\"labels\"], remainder]).astype(np.int64)\n",
        "                else:\n",
        "                    feature[\"labels\"] = np.concatenate([remainder, feature[\"labels\"]]).astype(np.int64)\n",
        "\n",
        "        features = self.tokenizer.pad(\n",
        "            features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=return_tensors,\n",
        "        )\n",
        "\n",
        "        # prepare decoder_input_ids\n",
        "        if (\n",
        "            labels is not None\n",
        "            and self.model is not None\n",
        "            and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\")\n",
        "        ):\n",
        "            decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(labels=features[\"labels\"])\n",
        "            features[\"decoder_input_ids\"] = decoder_input_ids\n",
        "\n",
        "        # ## now features are prompt generated by prompt selection model\n",
        "        # prompt_features = self.prompt_selection_policy.get_features_batch(features, forward_type = 'inference')\n",
        "        # prompt_features = self.tokenize_batch(prompt_features,features)\n",
        "\n",
        "        return features"
      ],
      "metadata": {
        "id": "gnU0QVPXsTIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "GVbyZ5rKUv6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AtJrvGK-a2nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wsc_training = pd.read_csv('drive/MyDrive/data/wsc_tr_0715.csv')"
      ],
      "metadata": {
        "id": "Bjde280Rsw9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wsc_training.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLL-UReXhZwZ",
        "outputId": "ec63d738-f7e1-43ad-95fb-0f10bfacf1e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['org_sample_idx', 'question_prompt', 'processed_text',\n",
              "       'generated_prompts', 'label', 'generated_prompt',\n",
              "       'question_prompt_input', 'org_input'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wsc_validation = pd.read_csv('drive/MyDrive/data/wsc_val_0715.csv')"
      ],
      "metadata": {
        "id": "rLqlo02AU0sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wsc_training.iloc[0,-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Haj7zJfPVRYb",
        "outputId": "09fae0b8-235c-40e9-a5cd-75fb62e755ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Passage: \\nMark told Pete many lies about himself, which Pete included in his book. *He* should have been more skeptical.\\nQuestion:\\n Determine the noun in the sentence to which the pronouns highlighted with * refer\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\", return_tensors=\"pt\")\n",
        "# model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
      ],
      "metadata": {
        "id": "f2Q4LsMLi2ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f76b94a-12a0-402e-8dce-d71f48379eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wsc_training['org_input'] += 'Answer:\\n '"
      ],
      "metadata": {
        "id": "VZjuKzi_Ffi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wsc_validation['org_input'] += 'Answer:\\n '"
      ],
      "metadata": {
        "id": "BcygNAuYFsfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class wscDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data,data_feature,tokenizer,data_seed=22):\n",
        "        # index_to_shuffle = data.index.copy().tolist()\n",
        "        # np.random.seed(data_seed)\n",
        "        # np.random.shuffle(index_to_shuffle)\n",
        "        # data = data.loc[index_to_shuffle,:].reset_index(drop=True)\n",
        "\n",
        "        self.input = tokenizer(data[data_feature].tolist(), truncation=True)\n",
        "\n",
        "        self.labels = [tokenizer(label, truncation=True)['input_ids'][:1] for label in data['label']]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item= {key: torch.tensor(val[idx]) for key, val in self.input.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = wscDataset(wsc_training, 'question_prompt_input', tokenizer)\n",
        "org_train_dataset = wscDataset(wsc_training.loc[wsc_training['org_input'].drop_duplicates().index,:], 'org_input', tokenizer)\n",
        "val_dataset = wscDataset(wsc_validation.loc[wsc_validation['org_input'].drop_duplicates().index,:], 'org_input', tokenizer)"
      ],
      "metadata": {
        "id": "Hi0DODokU30v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ntb-tZUYjIr7",
        "outputId": "bf221dfe-8c75-49b1-b023-f19dbc4e791a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[3424,  545,   10,  ...,    0,    0,    0],\n",
              "         [3424,  545,   10,  ...,    0,    0,    0],\n",
              "         [3424,  545,   10,  ...,    0,    0,    0],\n",
              "         [3424,  545,   10,  ...,    0,    0,    0],\n",
              "         [3424,  545,   10,  ...,    0,    0,    0]]),\n",
              " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
              " 'labels': [[2185], [2185], [2185], [2185], [2185]]}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_wsc_training_data(examples):\n",
        "     # encode a batch of sentences\n",
        "     encoding = tokenizer(examples[''], padding=\"max_length\", truncation=True)\n",
        "     # add labels as a list\n",
        "     encoding[\"labels\"] = tokenizer(examples['label'])['input_ids'][:1]\n",
        "     return encoding"
      ],
      "metadata": {
        "id": "fokhPMArh8O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_wsc_training_data(examples):\n",
        "     # encode a batch of sentences\n",
        "     encoding = tokenizer(f'org_input', padding=\"max_length\", truncation=True)\n",
        "     # add labels as a list\n",
        "     encoding[\"labels\"] = tokenizer(examples['label'])['input_ids'][:1]\n",
        "     return encoding"
      ],
      "metadata": {
        "id": "7Gc-dzQ3Uskk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_wsc_validation_data(examples):\n",
        "     # encode a batch of sentences\n",
        "     encoding = tokenizer(f'question_prompt_input', padding=\"max_length\", truncation=True)\n",
        "     # add labels as a list\n",
        "     encoding[\"labels\"] = tokenizer(examples['label'])['input_ids'][:1]\n",
        "     return encoding"
      ],
      "metadata": {
        "id": "CQxjPieQVfeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir t5-test"
      ],
      "metadata": {
        "id": "I7KsEOYmipaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e711814b-a485-4ed2-a3e2-d660ad8d52c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory t5-test: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "# Define training args\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir='t5-test',\n",
        "    per_device_train_batch_size =64,\n",
        "    per_device_eval_batch_size =64,\n",
        "    learning_rate=0.00005, # higher learning rate\n",
        "    max_steps =2000,\n",
        "    do_eval =True,\n",
        "    eval_steps = 100,\n",
        "    evaluation_strategy =\"steps\",\n",
        "    logging_dir=f\"t5-test/logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"tensorboard\",\n",
        ")"
      ],
      "metadata": {
        "id": "5tH3KzuKh0hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample_dat = load_dataset('super_glue','wic')"
      ],
      "metadata": {
        "id": "i7Nb-fXcdWYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(sample_dat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "3wYVUJm4uUID",
        "outputId": "df39a7d7-bca7-422e-9d49-2746a0c1c65b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-422e5ca02d39>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_dat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sample_dat' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batchdict_to_list(batch_dict):\n",
        "  batch_size = len(batch_dict[list(batch_dict.keys())[0]])\n",
        "  sample_list = [{k:v[i] for k,v in batch_dict.items()}for i in range(batch_size)]\n",
        "  return sample_list\n"
      ],
      "metadata": {
        "id": "j9jf-dON-J1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model"
      ],
      "metadata": {
        "id": "z9hAAfiy_XX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = custom_T5.from_pretrained(\"t5-base\")"
      ],
      "metadata": {
        "id": "jlz4LzzL-PEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator= FT_DataCollatorForSeq2Seq(tokenizer=tokenizer, model=test_model)"
      ],
      "metadata": {
        "id": "tjjZjUyl-fCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HhEdrj2GFArP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sample test"
      ],
      "metadata": {
        "id": "iWiS6Ti-Wou2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sample_dict = data_collator(batchdict_to_list(train_dataset[5:10]))"
      ],
      "metadata": {
        "id": "KUmLA_t0-WYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input_ids = test_model._shift_right(test_sample_dict['input_ids'])"
      ],
      "metadata": {
        "id": "G8n8psuoFDc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = test_sample_dict.pop('labels')"
      ],
      "metadata": {
        "id": "j6n4nBe-ESgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model.to('cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCIWA9xE-nuY",
        "outputId": "256f50e5-95b9-458f-d6dc-5f253847644b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "custom_T5(\n",
              "  (shared): Embedding(32128, 768)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-11): 11 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-11): 11 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sample_dict['decoder_input_ids'] = decoder_input_ids"
      ],
      "metadata": {
        "id": "UufUmCmBFIO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_out = test_model(**test_sample_dict,return_dict=True)"
      ],
      "metadata": {
        "id": "UVmXkz3z-nFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_out.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lykfpl7-By0y",
        "outputId": "76a0a8f8-12a8-43cd-ff9a-736c7317764c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['logits', 'past_key_values', 'decoder_hidden_states', 'encoder_last_hidden_state'])"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_out['decoder_hidden_states'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv-UktMOFQI9",
        "outputId": "2ee7f671-0a21-4ef8-b932-5e070778500f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 1, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_out['past_key_values'][-1][-1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtde-HqqB3fT",
        "outputId": "94b99f96-82dd-48c0-c39c-f80d62556bd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 12, 512, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_out['past_key_values'][-1][-2].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzOyKgVJCJdk",
        "outputId": "d303fb4b-d5d4-4c60-b251-31bcdf4ea46f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 12, 512, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_out['encoder_last_hidden_state'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkPX3zne-rKk",
        "outputId": "c181a8bb-1f8d-4f54-b3c7-cd31dae6d711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 512, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def preprocess_data(examples):\n",
        "#      # encode a batch of sentences\n",
        "#      encoding = tokenizer(f'Sentence1: {examples[\"sentence1\"]} Sentence2: {examples[\"sentence2\"]} Word: {examples[\"word\"]} Answer:', padding=\"max_length\", truncation=True)\n",
        "#      # add labels as a list\n",
        "#      encoding[\"labels\"] = tokenizer('True' if examples[\"label\"] == 1 else 'False')['input_ids'][:1]\n",
        "#      return encoding\n",
        "\n",
        "# # tokenize sentences + add labels\n",
        "# encoded_dataset = sample_dat.map(preprocess_data)\n",
        "# # turn into PyTorch dataset\n",
        "# encoded_dataset['train'].set_format(\"torch\",columns=['idx', 'label', 'input_ids', 'attention_mask', 'labels'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IequnDCOdhS0",
        "outputId": "d6e8e34e-47d3-421d-e782-84f342c172c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/super_glue/wic/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed/cache-a2c3caf1591c3bec.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/super_glue/wic/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed/cache-7085e21e0211d7ae.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/super_glue/wic/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed/cache-f69b97db338b3f83.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batchdict_to_list(batch_dict):\n",
        "  batch_size = len(batch_dict[list(batch_dict.keys())[0]])\n",
        "  sample_list = [{k:v[i] for k,v in batch_dict.items()}for i in range(batch_size)]\n",
        "  return sample_list\n"
      ],
      "metadata": {
        "id": "Y7ZbEEkC-OZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "J9H8XbVZuu59",
        "outputId": "1c260110-a4e7-48db-c4eb-7f070fceff8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-2189ee526292>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-58-1d96b99943b6>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mitem\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training"
      ],
      "metadata": {
        "id": "to1nO2o4WveC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = custom_T5.from_pretrained(\"t5-base\")"
      ],
      "metadata": {
        "id": "S_cIv04DOGb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator= FT_DataCollatorForSeq2Seq(tokenizer=tokenizer, model=test_model)"
      ],
      "metadata": {
        "id": "Bws9cM6ykpQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\", quiet=True)\n",
        "metric = evaluate.load('super_glue', 'wsc')\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "\n",
        "    preds, labels = eval_preds\n",
        "    # print(preds[0].argmax(-1).squeeze(-1).shape)\n",
        "    # print(labels.shape)\n",
        "\n",
        "    result = metric.compute(predictions=preds[0].argmax(-1).squeeze(-1), references=labels)\n",
        "    return result"
      ],
      "metadata": {
        "id": "g4WeANVTlM-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDbZk6pwNQBM",
        "outputId": "4784384f-c4f6-40c6-9e66-f621c703da79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "org_train_dataset"
      ],
      "metadata": {
        "id": "sLQydp4fpiK3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d40d750-fc54-49fc-a1e8-25ec9faf17c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.wscDataset at 0x78be0c5a44f0>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trail to test lr 0.00002, original text\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=test_model, args=training_args,tokenizer=tokenizer, train_dataset=train_dataset, eval_dataset=val_dataset,data_collator=data_collator, compute_metrics = compute_metrics)\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "id": "P_FuSmrotgS9",
        "outputId": "b0b732e3-747c-4672-afdc-55fef3016bcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2000/2000 11:54, Epoch 25/26]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.819300</td>\n",
              "      <td>2.240774</td>\n",
              "      <td>0.358491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.894800</td>\n",
              "      <td>2.166809</td>\n",
              "      <td>0.396226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.835700</td>\n",
              "      <td>2.342874</td>\n",
              "      <td>0.433962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.754000</td>\n",
              "      <td>2.375908</td>\n",
              "      <td>0.452830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.710200</td>\n",
              "      <td>2.407841</td>\n",
              "      <td>0.433962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.722200</td>\n",
              "      <td>2.504511</td>\n",
              "      <td>0.415094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.688100</td>\n",
              "      <td>2.426601</td>\n",
              "      <td>0.433962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.674600</td>\n",
              "      <td>2.564443</td>\n",
              "      <td>0.396226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.648500</td>\n",
              "      <td>2.588048</td>\n",
              "      <td>0.433962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.648100</td>\n",
              "      <td>2.579737</td>\n",
              "      <td>0.433962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.638500</td>\n",
              "      <td>2.596388</td>\n",
              "      <td>0.433962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.637000</td>\n",
              "      <td>2.548028</td>\n",
              "      <td>0.415094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.623000</td>\n",
              "      <td>2.480981</td>\n",
              "      <td>0.452830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.619900</td>\n",
              "      <td>2.544472</td>\n",
              "      <td>0.415094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.600300</td>\n",
              "      <td>2.695551</td>\n",
              "      <td>0.396226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.598800</td>\n",
              "      <td>2.655487</td>\n",
              "      <td>0.396226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.598900</td>\n",
              "      <td>2.615630</td>\n",
              "      <td>0.415094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.591300</td>\n",
              "      <td>2.632273</td>\n",
              "      <td>0.415094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.585500</td>\n",
              "      <td>2.656337</td>\n",
              "      <td>0.415094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.571000</td>\n",
              "      <td>2.659581</td>\n",
              "      <td>0.415094</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2000, training_loss=0.7229869155883789, metrics={'train_runtime': 715.2899, 'train_samples_per_second': 178.948, 'train_steps_per_second': 2.796, 'total_flos': 1.711487093630976e+16, 'train_loss': 0.7229869155883789, 'epoch': 25.97})"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trail to test lr 0.00002, original text\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model, args=training_args,tokenizer=tokenizer, train_dataset=encoded_dataset[\"train\"], eval_dataset=encoded_dataset[\"validation\"],data_collator=data_collator, compute_metrics = compute_metrics)\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "KGsQ2a1IWq61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "115b275c-67f7-4ab6-f06f-f3370566803f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-1395f9d49107>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#trail to test lr 0.00002, original text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m trainer = Seq2SeqTrainer(\n\u001b[0;32m----> 3\u001b[0;31m     model=model, args=training_args,tokenizer=tokenizer, train_dataset=encoded_dataset[\"train\"], eval_dataset=encoded_dataset[\"validation\"],data_collator=data_collator, compute_metrics = compute_metrics)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trail to test lr 0.00005, original text\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model, args=training_args,tokenizer=tokenizer, train_dataset=encoded_dataset[\"train\"], eval_dataset=encoded_dataset[\"validation\"],data_collator=data_collator, compute_metrics = compute_metrics)\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7kGn76MpTek2",
        "outputId": "7a99ed12-6f90-4709-ff67-9489fd946d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1110' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 1110/20000 16:26 < 4:40:23, 1.12 it/s, Epoch 6.52/118]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.625900</td>\n",
              "      <td>0.690388</td>\n",
              "      <td>0.670846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.496300</td>\n",
              "      <td>0.851804</td>\n",
              "      <td>0.653605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.443000</td>\n",
              "      <td>0.776496</td>\n",
              "      <td>0.659875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.387100</td>\n",
              "      <td>0.877174</td>\n",
              "      <td>0.655172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.361100</td>\n",
              "      <td>0.884124</td>\n",
              "      <td>0.659875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.261500</td>\n",
              "      <td>1.165649</td>\n",
              "      <td>0.659875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.274600</td>\n",
              "      <td>1.082122</td>\n",
              "      <td>0.683386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.208500</td>\n",
              "      <td>1.280255</td>\n",
              "      <td>0.681818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.198900</td>\n",
              "      <td>1.357498</td>\n",
              "      <td>0.672414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.149600</td>\n",
              "      <td>1.567481</td>\n",
              "      <td>0.681818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.130700</td>\n",
              "      <td>1.719165</td>\n",
              "      <td>0.673981</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m in \u001b[92m<cell line: 5>\u001b[0m:\u001b[94m5\u001b[0m                                                                              \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1645\u001b[0m in \u001b[92mtrain\u001b[0m                    \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m1642 \u001b[0m\u001b[2m      \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m1643 \u001b[0m\u001b[2m         \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m1644 \u001b[0m\u001b[2m      \u001b[0m)                                                                                 \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1645 \u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m1646 \u001b[0m\u001b[2m         \u001b[0margs=args,                                                                    \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m1647 \u001b[0m\u001b[2m         \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m1648 \u001b[0m\u001b[2m         \u001b[0mtrial=trial,                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1938\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m     \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m1935 \u001b[0m\u001b[2m               \u001b[0m\u001b[96mself\u001b[0m.control = \u001b[96mself\u001b[0m.callback_handler.on_step_begin(args, \u001b[96mself\u001b[0m.state,  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m1936 \u001b[0m\u001b[2m            \u001b[0m                                                                          \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m1937 \u001b[0m\u001b[2m            \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mself\u001b[0m.accelerator.accumulate(model):                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1938 \u001b[2m               \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                      \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m1939 \u001b[0m\u001b[2m            \u001b[0m                                                                          \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m1940 \u001b[0m\u001b[2m            \u001b[0m\u001b[94mif\u001b[0m (                                                                      \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m1941 \u001b[0m\u001b[2m               \u001b[0margs.logging_nan_inf_filter                                           \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2770\u001b[0m in \u001b[92mtraining_step\u001b[0m            \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2767 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mwith\u001b[0m amp.scale_loss(loss, \u001b[96mself\u001b[0m.optimizer) \u001b[94mas\u001b[0m scaled_loss:                     \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2768 \u001b[0m\u001b[2m            \u001b[0mscaled_loss.backward()                                                    \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2769 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m2770 \u001b[2m         \u001b[0m\u001b[96mself\u001b[0m.accelerator.backward(loss)                                               \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2771 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2772 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m loss.detach() / \u001b[96mself\u001b[0m.args.gradient_accumulation_steps                      \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2773 \u001b[0m                                                                                          \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/accelerate/\u001b[0m\u001b[1;33maccelerator.py\u001b[0m:\u001b[94m1821\u001b[0m in \u001b[92mbackward\u001b[0m               \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m1818 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melif\u001b[0m \u001b[96mself\u001b[0m.scaler \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                     \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m1819 \u001b[0m\u001b[2m         \u001b[0m\u001b[96mself\u001b[0m.scaler.scale(loss).backward(**kwargs)                                    \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m1820 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1821 \u001b[2m         \u001b[0mloss.backward(**kwargs)                                                       \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m1822 \u001b[0m\u001b[2m   \u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m1823 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92munscale_gradients\u001b[0m(\u001b[96mself\u001b[0m, optimizer=\u001b[94mNone\u001b[0m):                                          \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m1824 \u001b[0m\u001b[2;90m      \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m487\u001b[0m in \u001b[92mbackward\u001b[0m                         \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 484 \u001b[0m\u001b[2m            \u001b[0mcreate_graph=create_graph,                                                \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m            \u001b[0minputs=inputs,                                                            \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m         \u001b[0m)                                                                             \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 487 \u001b[2m      \u001b[0mtorch.autograd.backward(                                                          \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 488 \u001b[0m\u001b[2m         \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m      \u001b[0m)                                                                                 \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 490 \u001b[0m                                                                                          \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m200\u001b[0m in \u001b[92mbackward\u001b[0m               \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m200 \u001b[2m   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m      \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m      \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m203 \u001b[0m                                                                                           \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m\n",
              "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 5&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1645</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1642 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1643 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1644 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1645 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1646 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1647 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1648 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1938</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1935 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.control = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.callback_handler.on_step_begin(args, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.state,  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1936 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1937 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.accelerator.accumulate(model):                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1938 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1939 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1940 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> (                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1941 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>args.logging_nan_inf_filter                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2770</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training_step</span>            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2767 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> amp.scale_loss(loss, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.optimizer) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> scaled_loss:                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2768 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>scaled_loss.backward()                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2769 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>2770 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.accelerator.backward(loss)                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2771 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2772 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> loss.detach() / <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.gradient_accumulation_steps                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2773 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">accelerator.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1821</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1818 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1819 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.scale(loss).backward(**kwargs)                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1820 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1821 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>loss.backward(**kwargs)                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1822 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1823 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">unscale_gradients</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, optimizer=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>):                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1824 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">      </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">487</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 484 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 487 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 488 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">200</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>200 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trail to test lr 0.001\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model, args=training_args,tokenizer=tokenizer, train_dataset=encoded_dataset[\"train\"], eval_dataset=encoded_dataset[\"validation\"],data_collator=data_collator, compute_metrics = compute_metrics)\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        },
        "id": "JB6Yc_cg49EU",
        "outputId": "08ae89fa-bbca-4933-e11f-576919b34868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2107' max='262144' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  2107/262144 31:12 < 64:15:57, 1.12 it/s, Epoch 12.39/1543]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.824400</td>\n",
              "      <td>0.722975</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.712200</td>\n",
              "      <td>0.730823</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.722900</td>\n",
              "      <td>0.696544</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.703200</td>\n",
              "      <td>0.698106</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.689100</td>\n",
              "      <td>0.721196</td>\n",
              "      <td>0.498433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.642200</td>\n",
              "      <td>0.799563</td>\n",
              "      <td>0.543887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.618300</td>\n",
              "      <td>0.845689</td>\n",
              "      <td>0.579937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.558500</td>\n",
              "      <td>0.734143</td>\n",
              "      <td>0.572100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.514800</td>\n",
              "      <td>0.877713</td>\n",
              "      <td>0.579937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.457300</td>\n",
              "      <td>0.929826</td>\n",
              "      <td>0.561129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.397400</td>\n",
              "      <td>0.982053</td>\n",
              "      <td>0.562696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.379200</td>\n",
              "      <td>1.340612</td>\n",
              "      <td>0.572100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.310800</td>\n",
              "      <td>1.095136</td>\n",
              "      <td>0.542320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.299100</td>\n",
              "      <td>1.198050</td>\n",
              "      <td>0.561129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.263900</td>\n",
              "      <td>1.207139</td>\n",
              "      <td>0.567398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.216300</td>\n",
              "      <td>1.697970</td>\n",
              "      <td>0.581505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.229000</td>\n",
              "      <td>1.327175</td>\n",
              "      <td>0.564263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.165500</td>\n",
              "      <td>1.379512</td>\n",
              "      <td>0.556426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.170300</td>\n",
              "      <td>2.343281</td>\n",
              "      <td>0.564263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.135200</td>\n",
              "      <td>1.530170</td>\n",
              "      <td>0.572100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.136600</td>\n",
              "      <td>2.258003</td>\n",
              "      <td>0.536050</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "26hBKlnO5Lbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lZ5VbWoR5Lmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#positive, negative\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model, args=training_args,tokenizer=tokenizer, train_dataset=encoded_dataset[\"train\"], eval_dataset=encoded_dataset[\"validation\"],data_collator=data_collator, compute_metrics = compute_metrics)\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "I4KMC5ExgUb6",
        "outputId": "2af3cd8b-fc1e-429d-fef6-c23ee8838ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='541' max='262144' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   541/262144 07:57 < 64:23:17, 1.13 it/s, Epoch 3.18/1543]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.114800</td>\n",
              "      <td>0.694078</td>\n",
              "      <td>0.536050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.692400</td>\n",
              "      <td>0.701739</td>\n",
              "      <td>0.531348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.674000</td>\n",
              "      <td>0.709356</td>\n",
              "      <td>0.551724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.609200</td>\n",
              "      <td>0.745729</td>\n",
              "      <td>0.543887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.610600</td>\n",
              "      <td>0.799175</td>\n",
              "      <td>0.518809</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwiFEwk3cZFG",
        "outputId": "e4c5dde6-bd9c-49c3-819c-4c9457104d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jul  7 01:06:50 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    54W / 400W |  37763MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as palm\n",
        "\n",
        "palm.configure(api_key='AIzaSyDCGKnu8DQRxpQcgEGUp3wPvrHCIEazFhg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "7VjuYN4cc55U",
        "outputId": "a0df2037-daab-4f3a-ec71-ae5767d71eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/IPython/core/\u001b[0m\u001b[1;33minteractiveshell.py\u001b[0m:\u001b[94m3553\u001b[0m in \u001b[92mrun_code\u001b[0m        \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m3550 \u001b[0m\u001b[2m            \u001b[0m\u001b[94melif\u001b[0m async_ :                                                             \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m3551 \u001b[0m\u001b[2m               \u001b[0m\u001b[94mawait\u001b[0m \u001b[96meval\u001b[0m(code_obj, \u001b[96mself\u001b[0m.user_global_ns, \u001b[96mself\u001b[0m.user_ns)               \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m3552 \u001b[0m\u001b[2m            \u001b[0m\u001b[94melse\u001b[0m:                                                                     \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m3553 \u001b[2m               \u001b[0mexec(code_obj, \u001b[96mself\u001b[0m.user_global_ns, \u001b[96mself\u001b[0m.user_ns)                     \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m3554 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mfinally\u001b[0m:                                                                      \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m3555 \u001b[0m\u001b[2m            \u001b[0m\u001b[2m# Reset our crash handler in place\u001b[0m                                        \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m3556 \u001b[0m\u001b[2m            \u001b[0msys.excepthook = old_excepthook                                           \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m in \u001b[92m<cell line: 1>\u001b[0m:\u001b[94m1\u001b[0m                                                                              \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m\n",
              "\u001b[1;91mModuleNotFoundError: \u001b[0mNo module named \u001b[32m'google.generativeai'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/IPython/core/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">interactiveshell.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3553</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">run_code</span>        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3550 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> async_ :                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3551 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">await</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">eval</span>(code_obj, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_global_ns, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_ns)               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3552 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>3553 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>exec(code_obj, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_global_ns, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_ns)                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3554 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">finally</span>:                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3555 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Reset our crash handler in place</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3556 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>sys.excepthook = old_excepthook                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ModuleNotFoundError: </span>No module named <span style=\"color: #008000; text-decoration-color: #008000\">'google.generativeai'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_dat['train']"
      ],
      "metadata": {
        "id": "-KaPawlncQhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "\n",
        "train_policy_dict = {}\n",
        "for i in dat:\n",
        "  train_policy_dict.update()\n",
        "# the forward function automatically creates the correct decoder_input_ids\n",
        "loss = model(input_ids=input_ids, labels=labels).loss\n",
        "loss.item()"
      ],
      "metadata": {
        "id": "624mzGY3X4MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " transformers.Trainer"
      ],
      "metadata": {
        "id": "T0dv18g0Z_y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = T5ForConditionalGeneration('t5-base')"
      ],
      "metadata": {
        "id": "7JkhWrKvXiAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! watch -n 1 nvidia-smi"
      ],
      "metadata": {
        "id": "7eUqzDgWsJ-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "7z2p2x5gsaP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5xrHzqZ3tDAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "QtYSfQsJQMD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def evaluate_model(model_name: str):\n",
        "    # Load the T5 model and tokenizer\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "    # Load the ReCoRD dataset from the SuperGLUE benchmark\n",
        "    dataset = load_dataset(\"super_glue\", \"record\")\n",
        "    test_dataset = dataset[\"test\"]\n",
        "\n",
        "    # Prepare the QA pipeline\n",
        "    qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "    # Load the metric for ReCoRD\n",
        "    metric = load_metric(\"super_glue\", \"record\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    for sample in test_dataset:\n",
        "        print(sample)\n",
        "        print(sample.keys())\n",
        "        prediction = qa_pipeline({\n",
        "            \"context\": sample[\"passage\"],\n",
        "            \"question\": sample[\"query\"],\n",
        "        })\n",
        "        print(sample)\n",
        "        metric.add_batch(predictions=[prediction[\"answer\"]], references=[sample[\"answers\"]])\n",
        "\n",
        "    # Compute the final score\n",
        "    final_score = metric.compute()\n",
        "\n",
        "    return final_score"
      ],
      "metadata": {
        "id": "O3XTMkjtP7gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"super_glue\", \"record\")"
      ],
      "metadata": {
        "id": "6aHziB_i45xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['validation'][0]['passage'])"
      ],
      "metadata": {
        "id": "qjlZQB1l46oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['validation'][0].keys()"
      ],
      "metadata": {
        "id": "fDdNu3EO5zj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OQf5jI_m55b2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"t5-base\"  # replace with the T5 model name you want to evaluate\n",
        "dd = evaluate_model(model_name)"
      ],
      "metadata": {
        "id": "o-o2ve_sUKfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "super_glue_metric = datasets.load_metric('super_glue', 'record')\n",
        "predictions = [{'idx': {'passage': 0, 'query': 0}, 'prediction_text': 'answer'}]\n",
        "references = [{'idx': {'passage': 0, 'query': 0}, 'answers': ['answer', 'another_answer']}]\n",
        "results = super_glue_metric.compute(predictions=predictions, references=references)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "2ebUWwmTVNuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['validation'][0]['answers']"
      ],
      "metadata": {
        "id": "2r33cXrW7IjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "record_data_feature = {\n",
        "                \"predictions\": {\n",
        "                    \"idx\": {\n",
        "                        \"passage\": datasets.Value(\"int64\"),\n",
        "                        \"query\": datasets.Value(\"int64\"),\n",
        "                    },\n",
        "                    \"prediction_text\": datasets.Value(\"string\"),\n",
        "                },\n",
        "                \"references\": {\n",
        "                    \"idx\": {\n",
        "                        \"passage\": datasets.Value(\"int64\"),\n",
        "                        \"query\": datasets.Value(\"int64\"),\n",
        "                    },\n",
        "                    \"answers\": datasets.Sequence(datasets.Value(\"string\")),\n",
        "                },\n",
        "            }"
      ],
      "metadata": {
        "id": "4NXHGTbVW4ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = [\n",
        "    {\n",
        "        \"qas\": [\n",
        "            {\"id\": ref[\"idx\"][\"query\"], \"answers\": [{\"text\": ans} for ans in ref[\"answers\"]]}\n",
        "            for ref in references\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "predictions = {pred[\"idx\"][\"query\"]: pred[\"prediction_text\"] for pred in predictions}\n",
        "return evaluate_record(dataset, predictions)[0]"
      ],
      "metadata": {
        "id": "DaLs3qacXHBu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}